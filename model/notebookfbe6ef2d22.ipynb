{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-12T10:02:25.291057Z",
     "iopub.status.busy": "2026-01-12T10:02:25.290718Z",
     "iopub.status.idle": "2026-01-12T10:02:25.300787Z",
     "shell.execute_reply": "2026-01-12T10:02:25.300074Z",
     "shell.execute_reply.started": "2026-01-12T10:02:25.291029Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/wikitext2-data/test.txt\n",
      "/kaggle/input/wikitext2-data/train.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we need to normalize the text data (learning -->Learning both should be same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:02:25.302783Z",
     "iopub.status.busy": "2026-01-12T10:02:25.302466Z",
     "iopub.status.idle": "2026-01-12T10:02:25.324292Z",
     "shell.execute_reply": "2026-01-12T10:02:25.323665Z",
     "shell.execute_reply.started": "2026-01-12T10:02:25.302759Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:02:25.325440Z",
     "iopub.status.busy": "2026-01-12T10:02:25.325123Z",
     "iopub.status.idle": "2026-01-12T10:02:25.340961Z",
     "shell.execute_reply": "2026-01-12T10:02:25.340388Z",
     "shell.execute_reply.started": "2026-01-12T10:02:25.325402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalization(data):\n",
    "    return [line.lower().strip() for line in data if isinstance(line, str)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we need to convert the words into the tokens like \"i am learning deep learning\" --> [\"i\",\"am\",\"learning\",\"deep\",\"learning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:02:25.343141Z",
     "iopub.status.busy": "2026-01-12T10:02:25.342704Z",
     "iopub.status.idle": "2026-01-12T10:02:25.356938Z",
     "shell.execute_reply": "2026-01-12T10:02:25.356267Z",
     "shell.execute_reply.started": "2026-01-12T10:02:25.343118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def tokeinazation(data):\n",
    "#     return data.split()\n",
    "# i am using the huggnyface transformer so no need to manually to id it , will handle insode the it seld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now need to indexing the words so our model can learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:02:25.357970Z",
     "iopub.status.busy": "2026-01-12T10:02:25.357758Z",
     "iopub.status.idle": "2026-01-12T10:02:25.666368Z",
     "shell.execute_reply": "2026-01-12T10:02:25.665600Z",
     "shell.execute_reply.started": "2026-01-12T10:02:25.357950Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "MAX_LEN = 64  # cap sequence length to keep batches memory friendly\n",
    "def indexing(text_list):\n",
    "    \"\"\"\n",
    "    text_list: List[str]\n",
    "    returns: token_ids (Tensor), attention_mask (Tensor)\n",
    "    \"\"\"\n",
    "    encoded = tokenizer(\n",
    "        text_list,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encoded[\"input_ids\"], encoded[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:02:25.667703Z",
     "iopub.status.busy": "2026-01-12T10:02:25.667367Z",
     "iopub.status.idle": "2026-01-12T10:02:25.671924Z",
     "shell.execute_reply": "2026-01-12T10:02:25.671136Z",
     "shell.execute_reply.started": "2026-01-12T10:02:25.667665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# making the input data and the target data \n",
    "# we are divding the data into the input and the target \n",
    "# i am learning ---> data we have , now from this we are making the input like \n",
    "# if we put 'i' then the model should predict 'am' , like this , so for this we are mapping \n",
    "# the data sets , i -> am and more:\n",
    "def create_sequence(input_ids):\n",
    "    \"\"\"\n",
    "    input_ids: Tensor of shape (batch_size, seq_len)\n",
    "\n",
    "    Returns:\n",
    "        inputs  : (batch_size, seq_len-1)\n",
    "        targets : (batch_size, seq_len-1)\n",
    "    \"\"\"\n",
    "    inputs = input_ids[:, :-1]\n",
    "    targets = input_ids[:, 1:]\n",
    "    return inputs, targets\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the prepocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:02:25.673262Z",
     "iopub.status.busy": "2026-01-12T10:02:25.672907Z",
     "iopub.status.idle": "2026-01-12T10:02:29.050465Z",
     "shell.execute_reply": "2026-01-12T10:02:29.049730Z",
     "shell.execute_reply.started": "2026-01-12T10:02:25.673226Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed it\n",
      "passed it\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = pd.read_csv('/kaggle/input/wikitext2-data/train.txt', header=None)\n",
    "data = data[0].dropna().tolist()\n",
    "\n",
    "# normalization\n",
    "data = normalization(data)\n",
    "\n",
    "# indexing (GPT-2 tokenizer)\n",
    "input_ids, attention_mask = indexing(data)\n",
    "print(f\"Tokenized dataset shape: {input_ids.shape}\")\n",
    "print(\"Data tensors remain on CPU; move batches to GPU during training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:02:29.051610Z",
     "iopub.status.busy": "2026-01-12T10:02:29.051381Z",
     "iopub.status.idle": "2026-01-12T10:02:29.055048Z",
     "shell.execute_reply": "2026-01-12T10:02:29.054364Z",
     "shell.execute_reply.started": "2026-01-12T10:02:29.051587Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# inputs = [torch.tensor(seq) for seq in inputs]\n",
    "# targets = torch.tensor(targets)\n",
    "# padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LanguageModelingDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_ids.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = self.input_ids[idx]\n",
    "        mask = self.attention_mask[idx]\n",
    "        inputs = ids[:-1]\n",
    "        targets = ids[1:]\n",
    "        tgt_mask = mask[1:].to(torch.float32)\n",
    "        return inputs, targets, tgt_mask\n",
    "\n",
    "print(\"LanguageModelingDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = LanguageModelingDataset(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 16  # start small to keep GPU usage predictable\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=(device.type == \"cuda\")\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} samples.\")\n",
    "print(f\"DataLoader created with batch size {BATCH_SIZE}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:02:29.057446Z",
     "iopub.status.busy": "2026-01-12T10:02:29.057139Z",
     "iopub.status.idle": "2026-01-12T10:02:29.111741Z",
     "shell.execute_reply": "2026-01-12T10:02:29.111049Z",
     "shell.execute_reply.started": "2026-01-12T10:02:29.057411Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed it\n"
     ]
    }
   ],
   "source": [
    "embedding = torch.nn.Embedding(\n",
    "    num_embeddings=tokenizer.vocab_size,\n",
    "    embedding_dim=128,\n",
    "    padding_idx=tokenizer.pad_token_id\n",
    ")\n",
    "print(\"passed it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:02:29.112833Z",
     "iopub.status.busy": "2026-01-12T10:02:29.112629Z",
     "iopub.status.idle": "2026-01-12T10:02:29.118387Z",
     "shell.execute_reply": "2026-01-12T10:02:29.117647Z",
     "shell.execute_reply.started": "2026-01-12T10:02:29.112812Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, pad_token_id):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=pad_token_id\n",
    "        )\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids: (batch_size, seq_len)\n",
    "        returns logits: (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "\n",
    "        emb = self.embedding(input_ids)          # (B, T, E)\n",
    "        lstm_out, _ = self.lstm(emb)              # (B, T, H)\n",
    "        logits = self.fc(lstm_out)                # (B, T, V)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:02:29.119916Z",
     "iopub.status.busy": "2026-01-12T10:02:29.119573Z",
     "iopub.status.idle": "2026-01-12T10:02:29.304624Z",
     "shell.execute_reply": "2026-01-12T10:02:29.303801Z",
     "shell.execute_reply.started": "2026-01-12T10:02:29.119884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = LSTMModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:02:29.305856Z",
     "iopub.status.busy": "2026-01-12T10:02:29.305643Z",
     "iopub.status.idle": "2026-01-12T10:02:29.309964Z",
     "shell.execute_reply": "2026-01-12T10:02:29.309276Z",
     "shell.execute_reply.started": "2026-01-12T10:02:29.305835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T10:02:29.311206Z",
     "iopub.status.busy": "2026-01-12T10:02:29.310931Z",
     "iopub.status.idle": "2026-01-12T10:02:29.383304Z",
     "shell.execute_reply": "2026-01-12T10:02:29.382354Z",
     "shell.execute_reply.started": "2026-01-12T10:02:29.311182Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.43 GiB is free. Process 4067 has 13.31 GiB memory in use. Of the allocated memory 12.34 GiB is allocated by PyTorch, and 870.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/3860635753.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_55/2598881733.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# (B, T, E)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# (B, T, H)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# (B, T, V)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             result = _VF.lstm(\n\u001b[0m\u001b[1;32m   1125\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m                 \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.43 GiB is free. Process 4067 has 13.31 GiB memory in use. Of the allocated memory 12.34 GiB is allocated by PyTorch, and 870.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "batch_inputs, batch_targets, batch_mask = next(iter(dataloader))\n",
    "batch_inputs = batch_inputs.to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(batch_inputs)\n",
    "print(\"Forward pass on a single batch completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-12T10:02:29.383745Z",
     "iopub.status.idle": "2026-01-12T10:02:29.383971Z",
     "shell.execute_reply": "2026-01-12T10:02:29.383876Z",
     "shell.execute_reply.started": "2026-01-12T10:02:29.383862Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    token_count = 0.0\n",
    "    for batch_inputs, batch_targets, batch_mask in dataloader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        batch_mask = batch_mask.to(device)\n",
    "        valid_tokens = batch_mask.sum()\n",
    "        if valid_tokens == 0:\n",
    "            continue  # skip empty sequences\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
    "            logits = model(batch_inputs)\n",
    "            vocab_size = logits.size(-1)\n",
    "            per_token_loss = loss_fn(\n",
    "                logits.view(-1, vocab_size),\n",
    "                batch_targets.view(-1)\n",
    "            )\n",
    "            loss = (per_token_loss * batch_mask.view(-1)).sum() / valid_tokens\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item() * valid_tokens.item()\n",
    "        token_count += valid_tokens.item()\n",
    "    epoch_loss = total_loss / max(token_count, 1.0)\n",
    "    print(f\"Epoch {epoch + 1}: loss={epoch_loss:.4f}\")\n",
    "logits = logits.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-12T10:02:29.384757Z",
     "iopub.status.idle": "2026-01-12T10:02:29.384983Z",
     "shell.execute_reply": "2026-01-12T10:02:29.384889Z",
     "shell.execute_reply.started": "2026-01-12T10:02:29.384875Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # logits shape: (B, T, V)\n",
    "    # take the last timestep of the first example\n",
    "    last_logits = logits[0, -1]   # (V)\n",
    "\n",
    "    probs = torch.softmax(last_logits, dim=-1)\n",
    "    top_probs, top_ids = torch.topk(probs, k=5)\n",
    "\n",
    "    for prob, token_id in zip(top_probs, top_ids):\n",
    "        token = tokenizer.decode([token_id.item()])\n",
    "        print(f\"{token!r}  ->  {prob.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1480173,
     "sourceId": 2446022,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
